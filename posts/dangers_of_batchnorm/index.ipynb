{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Dangers of batch normalization\"\n",
    "subtitle: \"and what to use instaead\"\n",
    "format: html\n",
    "date: \"1/22/2023\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batchnorm is great - we don't need to worry about initialiation too much, exploding gradients are also not a problem any more! But it turns out that there are dangers to using batchnorm:\n",
    "\n",
    " 1. default momentum too small for small batch sizes, \n",
    " 2. it introduces state to the process and training forward pass all of the sudden work different than inference\n",
    "\n",
    "### ad. 1 default momentum\n",
    "As for the first one, it is just to remember, that we need to keep an eye on it and decrease momentum on small batches. The reason being that the running mean and std that are used inside batchnorm layers can jitter too much and it is mostly a problem at inference time. It can be skewed by the last training batch and diverge from the real dataset statistics. See this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ad. 2 additional state in the process\n",
    "\n",
    "This is a source of bugs reported by many people in the industry. Basically the running mean and std mentioned above is used differently during training and inference, so the process changes. If you forget to switch from training mode, to inference mode you have a problem, as your dataset statistics are screwed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "There are few alternatives to batchnorm layers:\n",
    " - group normalization\n",
    " - layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdaea57c0b9010ea74c77e884f0f49a414be4bd0fcb5deeb3a8fb91359695020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
