{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Training vision models on synthetic images\"\n",
    "description: Not SOTA performance, but quite good\n",
    "format: html\n",
    "date: \"01/14/2023\"\n",
    "categories: \n",
    "    - paper review\n",
    "    - vision\n",
    "image: front.png\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the paper (with code)](https://paperswithcode.com/paper/learning-to-see-by-looking-at-noise)\n",
    "\n",
    "Thanks to [Rosanne Liu](https://rosanneliu.com/) for presenting this paper at [Deep Learning: Classics and Trends](https://mlcollective.org/dlct/).\n",
    "\n",
    "Natural images, that we encounter in nature, occupy just a small space out of all possible variations of pixels. Surely training on a pure noise won't give us anything useful. However if we construct synthetic images that are closer to the real world images, can we train on it with some positive outcomes? Intuitively the answer is yes, but then how well can we train? Unless tagged otherwise, ideas in this blog post come from the paper.\n",
    "\n",
    "### Rationale for this exercise\n",
    "Models are more and more reliant on data. CLIP need **400 000 000** images to be trained. What if we could build a synthetic dataset to train? Why:\n",
    "\n",
    "- you don't have access to data\n",
    "- cheaper to maintain the data\n",
    "- maybe generating a good synthetic dataset can be better than real data (no human bias for example)\n",
    "\n",
    "### Task\n",
    "The training objective is classification of [ImageNet-100](https://www.kaggle.com/datasets/ambityga/imagenet100) images.\n",
    "\n",
    "### Training procedure\n",
    "It is done in 2 stages:\n",
    "\n",
    "1. The \"base\" network is trained using unsupervised contrastive learning (simplifying it is done by comparing similarity of images; [details](https://arxiv.org/abs/1807.03748v2))\n",
    "2. Final layer (but could be layers I think) are trained briefly on actual data to create a head of the model (to me it was not clear reading the paper but see [this training script](https://github.com/mbaradad/learning_with_noise/blob/main/align_uniform/linear_eval_imagenet100.py#L157) by authors)\n",
    "\n",
    "Technicalities derived from the [code](https://github.com/mbaradad/learning_with_noise):\n",
    "\n",
    "- unsupervised part:\n",
    "    - model parameters: TBD\n",
    "    - epochs: 200\n",
    "    - time: TBD\n",
    "- supervised part (training head for classification):\n",
    "    - model parameters: TBD\n",
    "    - epochs: 100\n",
    "    - time: TBD\n",
    "\n",
    "### Datasets\n",
    "\n",
    "See this image of datasets used in this experiment\n",
    "\n",
    "![](examples_of_sythetic_images.png)\n",
    "\n",
    "### Results\n",
    "\n",
    "![](results.png)\n",
    "\n",
    "Black bars are different baselines and coloured bars represent various sythetic datasets. About 20% is a difference between the best model trained on actual images and best model pre-trained on synthetic images.\n",
    "\n",
    "### My Conclusions\n",
    "Can this approach democratize access to data, as currently data collection and maintenance is being more and more concentrated? It looks like it, but for now it comes at the cost of performance. However what would be the result if data was generated by Stable Diffussion models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdaea57c0b9010ea74c77e884f0f49a414be4bd0fcb5deeb3a8fb91359695020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
